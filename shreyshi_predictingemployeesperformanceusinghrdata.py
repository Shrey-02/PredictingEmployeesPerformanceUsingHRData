# -*- coding: utf-8 -*-
"""Shreyshi_predictingEmployeesPerformanceUsingHRData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13u42nZyPpB0YdEI3viO-iviDOTMAqwDx

Importing all the required python libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder, StandardScaler

"""Loading the data

"""

data = pd.read_csv('/content/HR_dataset.csv')

"""Data **Exploration**"""

data.head()

data.shape

data.info()

data.describe()

"""Data **Preprocessing**"""

data.drop(
    ['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'],
    axis="columns",
    inplace=True
)

data.isnull().sum()

data=data.dropna()

data=data.drop_duplicates()

data.shape

#Feature Encoding
categorical_cols = data.select_dtypes(include='object').columns
label_encoder = LabelEncoder()
for col in categorical_cols:
    data[col] = label_encoder.fit_transform(data[col])

categorical_cols = data.select_dtypes(include='object').columns

#one-hot encoding
data = pd.get_dummies(data, columns=categorical_cols)

# Feature scaling
scaler = StandardScaler()
numerical_features = data.select_dtypes(include='number').columns
data[numerical_features] = scaler.fit_transform(data[numerical_features])

"""Data **Analysis**"""

plt.figure(figsize=(10,5))
plt.rc("font", size=14)
sns.countplot(x ='PerformanceRating',data=data)
plt.show()

plt.figure(figsize=(8,4))
sns.countplot(x='Gender',hue='PerformanceRating', data=data)
plt.title("PerformanceRating w.r.t Gender ")
plt.show()

plt.figure(figsize=(12,5))
sns.countplot(x='PercentSalaryHike',hue='PerformanceRating', data=data, palette='hot')
plt.title("PerformanceRating w.r.t PercentSalaryHike")
plt.xticks(rotation=45)
plt.show()

# Correlation heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm',annot_kws={"size":5})
plt.title('Correlation Heatmap')
plt.show()

data.corr()

# Calculating correlation of each feature with the 'PerformanceRating' column
correlation_with_target = data.corrwith(data['PerformanceRating'])


plt.barh(correlation_with_target.index, correlation_with_target.values)
plt.xlabel('Correlation with PerformanceRating')
plt.ylabel('Features')
plt.title('Correlation of Features with PerformanceRating')
plt.show()

"""Model **Selection**

Data **Splitting**
"""

X = data.drop(columns=['PerformanceRating'])
y = data['PerformanceRating']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""Model **Traning**"""

# Using Linear Regression as the model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)


# Using Random Forest Regression as the model
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

"""Model **Evalution**"""

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    return mse, rmse, r2

# Evaluating Linear Regression Model
linear_mse, linear_rmse, linear_r2 = evaluate_model(linear_model, X_test, y_test)

# Evaluating Random Forest Regression Model
rf_mse, rf_rmse, rf_r2 = evaluate_model(rf_model, X_test, y_test)

print("Linear Regression Model:")
print(f"Mean Squared Error (MSE): {linear_mse}")
print(f"Root Mean Squared Error (RMSE): {linear_rmse}")
print(f"R-squared (R²): {linear_r2}\n")

print("Random Forest Regression Model:")
print(f"Mean Squared Error (MSE): {rf_mse}")
print(f"Root Mean Squared Error (RMSE): {rf_rmse}")
print(f"R-squared (R²): {rf_r2}")

"""Model **Fine-tuning**"""

from sklearn.model_selection import GridSearchCV


param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=5,
                           scoring='neg_mean_squared_error',
                           n_jobs=-1)


grid_search.fit(X_train, y_train)


best_rf_model = grid_search.best_estimator_

# Evaluating the best model
best_rf_mse, best_rf_rmse, best_rf_r2 = evaluate_model(best_rf_model, X_test, y_test)

print("Best Random Forest Regression Model:")
print(f"Mean Squared Error (MSE): {best_rf_mse}")
print(f"Root Mean Squared Error (RMSE): {best_rf_rmse}")
print(f"R-squared (R²): {best_rf_r2}")

!pip install joblib

import joblib
best_model=best_rf_model
joblib.dump(best_rf_model, 'trained_model.joblib')

feature_importances = best_rf_model.feature_importances_


feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)

print(feature_importances_df)

feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)


plt.figure(figsize=(12, 6))
plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'])
plt.xscale('log')
plt.xlabel('Importance (Log Scale)')
plt.ylabel('Feature')
plt.title('Feature Importances from Random Forest Regression Model')
plt.tight_layout()
plt.show()

"""Loading the trained model"""

# Loading the trained model
rf_regressor = joblib.load('trained_model.joblib')

data = data[X_train.columns]

new_predictions = rf_regressor.predict(data)

print(new_predictions)

low_threshold = 2.0
high_threshold = 4.0


predicted_categories = np.where(new_predictions < low_threshold, 'Low',
                                np.where(new_predictions > high_threshold, 'High', 'Medium'))

category_counts = np.unique(predicted_categories, return_counts=True)

plt.bar(category_counts[0], category_counts[1])
plt.xlabel('Performance Categories')
plt.ylabel('Count')
plt.title('Predicted Performance Categories')
plt.show()